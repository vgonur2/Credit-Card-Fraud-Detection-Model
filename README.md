ğŸ¦ End-to-End Fraud Detection Pipeline (ML + Data Engineering)
ğŸ“Œ Project Overview

This project simulates a real-world bank fraud detection system using transaction data, machine learning, and a production-style data pipeline.

The goal is not just modeling, but demonstrating how fraud detection works end-to-end:

Data ingestion

Preprocessing

Model training

Threshold tuning

Batch inference

Database integration

Decision logic (auto-approve / manual review / auto-reject)

This mirrors how fraud models are deployed at large financial institutions (e.g., JPMorgan Chase, Capital One, Stripe).

ğŸ§  Problem Statement

Fraud detection is a highly imbalanced classification problem where:

Fraud is rare (<5%)

False positives are costly

Missed fraud is dangerous

Instead of optimizing for accuracy, this project focuses on:

Precision / Recall tradeoffs

Average Precision (PR-AUC)

Decision thresholds aligned with business rules

ğŸ“‚ Dataset

Based on Kaggle IEEE-CIS Fraud Detection dataset

~400+ anonymized features

Includes:

Transaction metadata

Card/device information

Time-based variables

Target label: isFraud

ğŸ—ï¸ Project Architecture
Raw CSV Data
     â†“
Data Cleaning & Feature Processing
     â†“
Preprocessor (Imputers + Encoder) â†’ saved as .pkl
     â†“
LightGBM Fraud Model â†’ saved as .pkl
     â†“
SQLite Mock Bank Database
     â†“
Batch Scoring Pipeline
     â†“
Fraud Decision Engine

âš™ï¸ Tech Stack

Languages & Libraries

Python

Pandas, NumPy

Scikit-learn

LightGBM

Joblib

SQLite3

Concepts Used

Class imbalance handling

Ordinal encoding

Missing value imputation

Model persistence

Batch inference

SQL â†” ML integration

Business-driven thresholds

ğŸ”¬ Modeling Approach
Model

LightGBM (Gradient Boosted Trees)

Chosen because:

Handles large feature spaces

Performs well on tabular fraud data

Used widely in industry

Training Strategy

Time-aware train / test split

Class imbalance handled via:

scale_pos_weight = num_nonfraud / num_fraud


Evaluation metric:

Average Precision Score (PR-AUC)

ğŸ¯ Threshold-Based Decision System

Instead of a single prediction, the model outputs a fraud probability, which is mapped to actions:

Fraud Probability	Decision
â‰¥ 0.90	AUTO REJECT
0.70 â€“ 0.89	MANUAL REVIEW
< 0.70	AUTO APPROVE

This reflects how real banks operate â€” ML assists humans, it doesnâ€™t blindly replace them.

ğŸ—„ï¸ Mock Bank Database (SQLite)

A SQLite database simulates a production banking system:

Transactions stored in transactions table

Batch processing using LIMIT / OFFSET

Each transaction is:

Pulled from SQL

Preprocessed using saved pipeline

Scored by ML model

Assigned a fraud decision

This demonstrates data engineering + ML integration, not just notebooks.

ğŸ”„ Batch Scoring Pipeline
SELECT * FROM transactions
LIMIT batch_size OFFSET offset


For each batch:

Load from database

Apply saved preprocessor

Predict fraud probability

Apply decision rules

Output results (or store back to DB)

This simulates real-world fraud monitoring systems.

ğŸ“ Repository Structure
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_eda.ipynb
â”‚   â”œâ”€â”€ 02_feature_engineering.ipynb
â”‚   â”œâ”€â”€ 03_model_training.ipynb
â”‚   â”œâ”€â”€ 04_threshold_analysis.ipynb
â”‚
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ batch_scoring.py
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ fraud_model.pkl
â”‚   â”œâ”€â”€ preprocessor.pkl
â”‚
â”œâ”€â”€ mock_database_bank.db
â”œâ”€â”€ README.md

ğŸš€ How to Run
1ï¸âƒ£ Train the Model

Run the training notebooks to generate:

fraud_model.pkl

preprocessor.pkl

2ï¸âƒ£ Create Database
python pipeline/create_database.py

3ï¸âƒ£ Run Batch Scoring
python pipeline/batch_scoring.py

ğŸ“ˆ Sample Output
TransactionID | Fraud Probability | Decision
--------------------------------------------
3032075       | 0.9426            | AUTO REJECT
3032078       | 0.7585            | MANUAL REVIEW
3032070       | 0.0098            | AUTO APPROVE

ğŸ§  Key Learnings

Fraud detection is not about accuracy

Thresholds matter more than models

ML must integrate with databases and pipelines

Interpretability is often traded for performance

Production ML = engineering + modeling

ğŸ”® Future Improvements

Real-time streaming (Kafka)

REST API (FastAPI)

Model monitoring & drift detection

SHAP explainability dashboards

Cloud deployment (AWS/GCP)

ğŸ‘¤ Author

Vishruth Gonur
Information Science + Data Science
University of Illinois Urbana-Champaign
